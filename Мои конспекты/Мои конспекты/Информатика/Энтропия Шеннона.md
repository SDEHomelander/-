#Измерение_информации #Информатика #Информационная_теория #Мера_информации #Энтропия_Шеннона

Энтропия Шеннона - это мера неопределенности или случайности в информации. Она измеряет количество информации, которое необходимо для описания сообщения или события.

Формула для расчета энтропии Шеннона:

H = - ∑ (p(x) * log2(p(x)))

Где:

- H - энтропия Шеннона
- p(x) - вероятность события x
- log2 - логарифм по основанию 2

Простыми словами, энтропия Шеннона измеряет, сколько информации необходимо для описания события или сообщения. Чем выше энтропия, тем больше информации необходимо для описания события.

Например, если у нас есть сообщение, которое может принимать два значения: "0" или "1", то энтропия Шеннона будет равна 1 бит, потому что для описания этого сообщения необходимо только один бит информации.

Если у нас есть сообщение, которое может принимать четыре значения: "00", "01", "10" или "11", то энтропия Шеннона будет равна 2 бита, потому что для описания этого сообщения необходимо два бита информации.

Энтропия Шеннона имеет важное значение в информационной теории и криптографии, потому что она помогает измерять количество информации, которое необходимо для передачи или хранения данных.